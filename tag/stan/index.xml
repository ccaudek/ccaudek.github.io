<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>stan | Welcome to my Website</title>
    <link>/tag/stan/</link>
      <atom:link href="/tag/stan/index.xml" rel="self" type="application/rss+xml" />
    <description>stan</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 27 Nov 2016 10:06:00 -0500</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>stan</title>
      <link>/tag/stan/</link>
    </image>
    
    <item>
      <title>Bayesian modeling for Classic Test Theory</title>
      <link>/post/2016/stan/</link>
      <pubDate>Sun, 27 Nov 2016 10:06:00 -0500</pubDate>
      <guid>/post/2016/stan/</guid>
      <description>&lt;p&gt;&lt;em&gt;Bayesian Psychometric Modeling&lt;/em&gt; by Levy and Mislevy (2016) provides a nice discussion of Bayesian modeling for the Classic Test Theory (CTT). The classic references are Lord and Novick (1968) and McDonald (1999).&lt;/p&gt;
&lt;p&gt;Initially, Levy and Mislevy discuss the CTT by considering a single test and assuming that the measurement model parameters and the hyperparameters are known. Then they generalize to the case where there are multiple tests, still assuming that the measurement model parameters and hyperparameters are known. Finally, they consider the case where the measurement model parameters and hyperparameters are unknown. In their book, Levy and Mislevy use WinBUGS but here I re-write one of their models in Stan. I will consider the case in which the measurement model parameters and hyperparameters are unknown.&lt;/p&gt;
&lt;p&gt;For each examinee &lt;code&gt;$i$&lt;/code&gt;, &lt;code&gt;$x_i$&lt;/code&gt; is the observable test score. The test can be made up by several items and the test score is the sum of the responses to the individual items. According to CTT, &lt;code&gt;$x_i$&lt;/code&gt; is the sum of a true score (for each examinee) and of an error component:&lt;/p&gt;
&lt;div&gt;$$x_i = T_i + E_i,$$&lt;/div&gt;
where `$T_i$` is the true score for examinee `$i$`, with mean `$\mu$` and variance `$\sigma_T^2$` and `$E$` is the  error component for examinee `$i$`, with mean 0 and variance  `$\sigma_E^2$`.  Errors are assumed to be uncorrelated with true scores in the population,
&lt;div&gt;$$\rho_{TE} = 0.$$&lt;/div&gt;
&lt;p&gt;The variance of the observed scores, &lt;code&gt;$\sigma^2_x$&lt;/code&gt;, is the sum of the true variance and of the error variance,&lt;/p&gt;
&lt;div&gt;$$\sigma^2_x = \sigma^2_T + \sigma^2_E,$$&lt;/div&gt;
&lt;p&gt;from which the reliability of the test is given by the ratio between the variance of the true scores and the variance of the observed scores (that is, the proportion of
observed score variance that is accounted for by the true score variance):&lt;/p&gt;
&lt;div&gt;$$\rho = \frac{\sigma^2_T}{\sigma^2_x}.$$&lt;/div&gt;
&lt;p&gt;The reliability &lt;code&gt;$\rho$&lt;/code&gt; also corresponds to the squared correlation between the observed scores and the true scores, &lt;code&gt;$\rho = \rho_{xT}^2$&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Levy and Mislevy (2016) discuss how the true scores can be estimated with Kelley&amp;rsquo;s formula (Kelley, 1923).  Kelley&amp;rsquo;s formula shifts the estimate of each examinee&amp;rsquo;s true score in the direction of the sample mean by a measure which depends on the reliability of the test. If &lt;code&gt;$\rho = 1$&lt;/code&gt; the observed score becomes the estimate of the true score; if &lt;code&gt;$\rho = 0$&lt;/code&gt;, &lt;code&gt;$\mu_x$&lt;/code&gt; becomes the best estimate of the true score. The best estimate of the true score will therefore fall between &lt;code&gt;$x_i$&lt;/code&gt; and &lt;code&gt;$\mu_x$&lt;/code&gt;, and it will be closer to &lt;code&gt;$x_i$&lt;/code&gt; if the reliability of the test is higher.&lt;/p&gt;
&lt;p&gt;Levy and Mislevy (2016) show that the posterior mean obtained from a Bayesian implementation of the CTT is consistent with Kelley&amp;rsquo;s formula. They provide a formal proof of this, but here I will only replicate the MCMC estimation via Stan. I will consider   here their last example, in which they consider the case of the CTT with an unknown measurement model. Specifically, Levy and Mislevy analyze the data of 10 examinees, each of them with 5 observable measures. This can be understood as the case in which the same test is administered to each examinee on five occasions, with no effects of learning or fatigue.&lt;/p&gt;
&lt;p&gt;I focus here on the case in which &lt;code&gt;$\mu_T$&lt;/code&gt;, &lt;code&gt;$\sigma_T^2$&lt;/code&gt;, and &lt;code&gt;$\sigma_E^2$&lt;/code&gt; are unknown. The purpose is to estimate the reliability of the test and to estimate the true scores of the examinees by using only the observed scores.&lt;/p&gt;
&lt;p&gt;I have re-written in Stan the WinBUGS model that Levy and Mislevy (2016) present in section 8.3.3.3. Since I will use &lt;code&gt;rstan&lt;/code&gt;, I saved the following code in a file called &lt;code&gt;ctt_2.stan&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data {
  int&amp;lt;lower=0&amp;gt; N; //the number of subjects (rows in the X matrix)
  int&amp;lt;lower=0&amp;gt; J; //the number of columns in the X matrix
  matrix[N, J] X; //the data matrix
}
parameters {
  vector[N] T; //the true scores
  real mu_T; // mean of the distribution of true scores
  real&amp;lt;lower=0&amp;gt; sigma_T; // sd of the distribution of true scores
  real&amp;lt;lower=0&amp;gt; sigma_E; // sd of the distribution of errors
}
transformed parameters {
}
model {
  // Priors
  mu_T ~ normal(0, 200);
  sigma_T ~ normal(0, 20);
  sigma_E ~ normal(0, 20);
  // Likelihood
  for (i in 1:N) {
      T[i] ~ normal(mu_T, sigma_T);
      for (j in 1:J) {
          X[i, j] ~ normal(T[i], sigma_E);
      }
  }
}
generated quantities {
  real&amp;lt;lower=0&amp;gt; reliability;
  real&amp;lt;lower=0&amp;gt; reliability_composite;

  reliability = sigma_T^2 / (sigma_T^2 + sigma_E^2);
  reliability_composite = J * reliability / ((J - 1) * reliability + 1);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Differently from Levy and Mislevy (2016), who used priors for &lt;code&gt;$\mu_T$&lt;/code&gt;, &lt;code&gt;$\sigma_T$&lt;/code&gt;, and &lt;code&gt;$\sigma_E$&lt;/code&gt; centered on the correct values, I use much less informative priors in order to test the robustness of the estimation. I will not discuss here the details of the code. It is easy to find many introductions of Stan on the web. Let see instead how we can run this code in &lt;code&gt;R&lt;/code&gt;.  I used the following instructions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;rm(list = ls(all = TRUE))
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The previous lines of code will load Stan in the &lt;code&gt;R&lt;/code&gt; environment after clearing the workspace. With the following instructions, I create a list with the data in the appropriate format for Stan. &lt;em&gt;N&lt;/em&gt; is the number of examinees, &lt;em&gt;J&lt;/em&gt; is the number of administrations of the test for each examinee, and &lt;em&gt;X&lt;/em&gt; is the data matrix. Each row of &lt;em&gt;X&lt;/em&gt; contains the results of one examinee, with the columns providing the results of the repeated administrations of the test.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;stan_dat &amp;lt;- list(
  N = 10,
  J = 5,
  X = matrix(
    c(
      80, 77, 80, 73, 73,
      83, 79, 78, 78, 77,
      85, 77, 88, 81, 80,
      76, 76, 76, 78, 67,
      70, 69, 73, 71, 77,
      87, 89, 92, 91, 87,
      76, 75, 79, 80, 75,
      86, 75, 80, 80, 82,
      84, 79, 79, 77, 82,
      96, 85, 91, 87, 90
    ), nrow = 10, ncol = 5, byrow = TRUE)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I fitted the model contained in the &lt;code&gt;ctt_2.stan&lt;/code&gt; file as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;fit &amp;lt;- stan(file = &amp;quot;ctt_2.stan&amp;quot;,
               data = stan_dat,
               pars = c(&amp;quot;T&amp;quot;, &amp;quot;mu_T&amp;quot;, &amp;quot;sigma_T&amp;quot;,
                        &amp;quot;sigma_E&amp;quot;, &amp;quot;reliability&amp;quot;,
                        &amp;quot;reliability_composite&amp;quot;),
               iter = 20000,
               chains = 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the results:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Inference for Stan model: ctt_2.
4 chains, each with iter=20000; warmup=10000; thin=1;
post-warmup draws per chain=10000, total post-warmup draws=40000.

                         mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff Rhat
T[1]                    76.85    0.01 1.55   73.85   75.81   76.84   77.87   79.92 32011    1
T[2]                    79.07    0.01 1.54   76.05   78.04   79.07   80.08   82.12 33954    1
T[3]                    82.06    0.01 1.52   79.09   81.03   82.06   83.08   85.03 32860    1
T[4]                    74.99    0.01 1.53   71.96   73.97   74.99   76.00   78.01 32862    1
T[5]                    72.57    0.01 1.56   69.51   71.53   72.55   73.59   75.71 32570    1
T[6]                    88.55    0.01 1.57   85.41   87.53   88.58   89.60   91.58 31220    1
T[7]                    77.20    0.01 1.52   74.23   76.18   77.20   78.20   80.21 32944    1
T[8]                    80.58    0.01 1.52   77.57   79.57   80.58   81.59   83.56 33246    1
T[9]                    80.19    0.01 1.53   77.18   79.16   80.18   81.20   83.21 33681    1
T[10]                   89.12    0.01 1.58   85.94   88.07   89.14   90.18   92.17 31818    1
mu_T                    80.09    0.01 2.17   75.80   78.76   80.09   81.43   84.40 24525    1
sigma_T                  6.47    0.01 1.94    3.80    5.14    6.12    7.40   11.20 21723    1
sigma_E                  3.56    0.00 0.41    2.86    3.26    3.52    3.81    4.48 23602    1
reliability              0.74    0.00 0.11    0.50    0.67    0.75    0.82    0.92 24608    1
reliability_composite    0.93    0.00 0.04    0.83    0.91    0.94    0.96    0.98 25083    1
lp__                  -107.17    0.02 2.83 -113.67 -108.86 -106.81 -105.12 -102.70 14242    1

Samples were drawn using NUTS(diag_e) at Sun Nov 27 09:08:53 2016.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at
convergence, Rhat=1).
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Stan implementation of Levy and Mislevy&amp;rsquo;s model works. For example, for the estimate of the true score of the first examinee, Levy and Mislevy (2016) report a mean of 76.85, a standard deviation of 1.53, and a 95% CI of &lt;code&gt;$[73.87, 79.83]$&lt;/code&gt;. With much more generic priors, I find a mean of 76.85, a standard deviation of 1.55, and a 95% CI of &lt;code&gt;$[73.85, 79.92]$&lt;/code&gt;. Levy and Mislevy report a reliability of the composite score of 0.93, with a standard deviation of 0.04, and a 95% CI of &lt;code&gt;$[0.86, 0.98]$&lt;/code&gt;. I find a reliability of 0.94, with a standard deviation of 0.04, and a 95% CI of &lt;code&gt;$[0.83, 0.98]$&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It is interesting that, even when almost uninformative priors are used, correct estimates for the parameters can be obtained.&lt;/p&gt;
&lt;script type=&#34;text/javascript&#34;
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;
&lt;script type=&#34;text/x-mathjax-config&#34;&gt;
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]],
    displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\[&#39;,&#39;\]&#39;]],
    processEscapes: true,
    processEnvironments: true,
    skipTags: [&#39;script&#39;, &#39;noscript&#39;, &#39;style&#39;, &#39;textarea&#39;, &#39;pre&#39;],
    TeX: { equationNumbers: { autoNumber: &#34;AMS&#34; },
         extensions: [&#34;AMSmath.js&#34;, &#34;AMSsymbols.js&#34;] }
  }
});
&lt;/script&gt;
&lt;script type=&#34;text/x-mathjax-config&#34;&gt;
  MathJax.Hub.Queue(function() {
    // Fix &lt;code&gt; tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i &lt; all.length; i += 1) {
        all[i].SourceElement().parentNode.className += &#39; has-jax&#39;;
    }
});
&lt;/script&gt;
</description>
    </item>
    
  </channel>
</rss>
