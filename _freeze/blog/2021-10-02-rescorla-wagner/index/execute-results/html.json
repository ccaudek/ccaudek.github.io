{
  "hash": "35ebf2ba8f8478e5b7184dcb41493907",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Rescorla-Wagner\"\ndescription: |\n  Un semplice modello di apprendimento associativo.\nauthor:\n  - name: Corrado Caudek\n    url: {}\ndate: 10-02-2021\noutput:\n  distill::distill_article:\n    self_contained: false\ncategories:\n  - LM-51\n  - R\n---\n\n\n\n\n\n\n# Regola di Rescorla-Wagner\n\nIl modello di Rescorla-Wagner fornisce una regola di apprendimento che descrive come cambia la forza associativa durante il condizionamento pavloviano. Supponiamo di prendere uno stimolo inizialmente neutro (ad es. un tono) e di associarlo a un risultato che ha un valore intrinseco per l'organismo (ad es. un premio -- oppure una punizione). Col tempo l'organismo impara ad associare il tono al premio e risponderà al tono più o meno allo stesso modo in cui risponde al premio. In questo esempio il premio è lo stimolo incondizionato (US) e il tono è stimolo condizionato (SC).\n\nSecondo il modello Rescorla-Wagner, la regola per l'aggiornamento della forza associativa tra US e SC è basata sul divario tra l'aspettativa di ricompensa e il risultato che viene effettivamente ottenuto:\n\n$$\nv_{s,t} = v_{s,t-1} + \\alpha \\cdot (\\lambda_{t-1} - v_{s,t-1}),\n$$\ndove\n\n- $v_{s,t}$ è il valore dello stimolo $s$ nella prova $t$, che riflette l'aspettativa di una ricompensa,\n- $\\lambda_{t-1}$ è la ricompensa ricevuta nella prova $t-1$,\n- $\\alpha$ è il tasso di apprendimento.\n\nPertanto, il valore assegnato ad uno stimolo viene aggiornato in base all'errore di previsione (la differenza tra il feedback ricevuto $\\lambda_{t-1}$ e l'aspettativa di ricompensa $v_{s,t-1}$).\n\nIl tasso di apprendimento $\\alpha \\in [0, 1]$ determina quanto viene pesato questo errore di previsione nell'aggiornamento dell'aspettativa di ricompensa alla luce del feedback che è stato ottenuto.\n\n\n# Condizionamento\n\nPer chiarire il funzionamento della regola di Rescorla-Wagner la implementiamo in una funzione `R`:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nupdate_rw <- function(value, alpha=0.15, lambda=1) {\n  value + alpha * (lambda - value)\n}\n```\n:::\n\n\n\n\nIn una prima simulazione costituita da una sequenza di 40 prove esaminiamo come varia  l'aspettativa di ricompensa dello stimolo $s$ nel tempo. Immaginiamo che il feedback ottenuto sia sempre pari ad una ricompensa ($\\lambda = 1$). Nella prima prova, il valore dello stimolo è inizializzato a zero.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn_trials <- 40\nstrength <- numeric(n_trials)\nstrength\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[39] 0 0\n```\n\n\n:::\n\n```{.r .cell-code}\nfor(trial in 2:n_trials) {\n  strength[trial] <- update_rw( strength[trial-1] )\n}\nprint(strength)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0.0000000 0.1500000 0.2775000 0.3858750 0.4779937 0.5562947 0.6228505\n [8] 0.6794229 0.7275095 0.7683831 0.8031256 0.8326568 0.8577582 0.8790945\n[15] 0.8972303 0.9126458 0.9257489 0.9368866 0.9463536 0.9544006 0.9612405\n[22] 0.9670544 0.9719962 0.9761968 0.9797673 0.9828022 0.9853819 0.9875746\n[29] 0.9894384 0.9910226 0.9923692 0.9935139 0.9944868 0.9953138 0.9960167\n[36] 0.9966142 0.9971221 0.9975538 0.9979207 0.9982326\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(\n  1:n_trials, \n  strength, \n  type = 'l', \n  ylim = c(0,1),\n  xlab = \"Prove\",\n  ylab = \"Aspettativa di ricompensa\")\npoints(1:n_trials, strength)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\nApplicando la regola di Rescorla-Wagner, il valore (ovvero, l'aspettativa di ricompensa) dello stimolo $s$, nel caso di feedback positivi, aumenta progressivamente fino a raggiungere l'asintoto di 1. Nella simulazione precedente abbiamo posto $\\alpha = 0.15$.  Con $\\alpha = 0.5$ otteniamo:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstrength <- numeric(n_trials)\n\nfor(trial in 2:n_trials) {\n  strength[trial] <- update_rw(alpha = 0.5, strength[trial-1] )\n}\nplot(\n  1:n_trials, \n  strength, \n  type = 'l', \n  ylim = c(0,1),\n  xlab = \"Prove\",\n  ylab = \"Aspettativa di ricompensa\"\n)\npoints(1:n_trials, strength)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\nÈ chiaro dunque che il parametro $\\alpha$ determina la velocità con la quale viene aggiornata l'aspettativa di ricompensa.\n\n\n# Estinzione\n\nConsideriamo ora l'estinzione dell'associazione che è stata appresa. In questa seconda simulazione, le prime 25 prove saranno identiche a quelle della simulazione precedente. In esse verrà sempre fornita una ricompensa ($\\lambda = 1)$. Le ultime 25 prove, invece, forniranno un feedback negativo, ovvero, $\\lambda = 0$ -- possiamo immaginare il feedback come l'assenza di premio.\n\nQuello che ci aspettiamo di vedere in questa situazione è che dopo la prova 25, quando il premio viene rimosso, la forza dell'associazione inizi a indebolirsi perché l'agente sta ora associando il CS con l'assenza di premio (cioè il parametro $\\lambda$ è sceso a zero e quindi l'associazione $v$ ritorna lentamente al valore iniziale). \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn_trials <- 50                \nstrength <- numeric(n_trials) \nlambda <- 1 # initial reward value \n\nfor(trial in 2:n_trials) {\n  \n  # remove the shock after trial 25\n  if(trial > 25) {\n    lambda <- 0\n  }\n  \n  # update associative strength on each trial\n  strength[trial] <- update_rw(\n    value = strength[trial-1],\n    lambda = lambda\n  )\n}\n\nplot(\n  1:n_trials, \n  strength, \n  type = 'l', \n  ylim = c(0,1),\n  xlab = \"Prove\",\n  ylab = \"Aspettativa di ricompensa\"\n)\npoints(1:n_trials, strength)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\nL'estinzione è efficace nel rimuovere l'associazione, ma la sua efficacia richiede del tempo, non è immediata. Se ci fermiamo alla 35-esima prova, per esempio, allo stimolo $s$ sarà ancora associata una piccola aspettativa di ricompensa.\n\n\n# Regola soft-max\n\nUna volta attribuita una aspettativa di ricompensa agli stimoli, l'agente deve scegliere tra i diversi stimoli che sono presenti. Potrebbe sembrare ovvio scegliere, tra i vari stimoli presenti, quello a cui è associata l'aspettativa di ricompensa più altra (\"massimizzazione della probabilità\") in questo particolare compito. Ma gli organismi biologici non si comportano così. Piuttosto, tendono a scegliere più spesso lo stimolo a cui è associata l'aspettativa di ricompensa maggiore, ma non sempre. Ci sono marcate differenze individuali nella strategia di scelta che si colloca tra due estremi: un'estremo è quello in cui l'aspettativa di valore determina la scelta; l'altro estremo è quello in cui la scelta tra gli stimoli è puramente casuale (ovvero, non è in alcun modo determinata dall'aspettativa di ricompensa associata agli stimoli). \n\nPer descrivere il continuum tra queste due diverse strategie di scelta\n\nPer modellare il modo in cui gli agenti traducono i valori di aspettativa di ricompensa in una scelta, viene utilizzato un modello in grado di catturare queste diverse possibili strategie di scelta. A questo fine viene usata la cosiddetta equazione soft-max:\n\n$$\np(s) = \\frac{\\exp(\\beta v_s)}{\\sum_i \\exp(\\beta v_i)}.\n$$\nSe supponiamo che ci siano solo due stimoli, A e B, dove $v_B = 1 - v_A$, allora otteniamo la situazione seguente.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsoftmax <- function(beta, x) {\n  1 / (1 + exp(-beta * x))\n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbeta <- 5\nx <- seq(-1, 1, length.out = 100)\ny <- softmax(beta, x)\nplot(\n  x, \n  y, \n  type = 'l', \n  #ylim = c(0,1),\n  xlab = \"Valore (A) - valore (B)\",\n  ylab = \"p(scelta = A)\"\n)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\nSi noti che\n\n- La probabilità di scegliere lo stimolo A aumenta in modo monotono con la differenza di valore A - B.\n- La funzione softmax ci dice che l'agente sceglierà lo stimolo A la maggior parte delle volte quando $v_A > v_B$, ma non sempre.\n- Da qui deriva il termine 'softmax': l'agente sceglie lo stimolo con il valore maggiore la maggior parte delle volte (ma non sempre), quindi questa è una funzione di massimizzazione 'soft'.\n\n\n### Informazioni sulla sessione di lavoro {-}\n\n<details>\n<summary>\nSession Info\n</summary>\nSono qui fornite le informazioni sulla sessione di lavoro insieme all'elenco dei pacchetti usati. I pacchetti contrassegnati con un asterisco(*) sono stati usati esplicitamente nello script.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.2 (2024-10-31)\n os       macOS Sequoia 15.2\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Rome\n date     2025-01-26\n pandoc   3.6.2 @ /opt/homebrew/bin/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cli           3.6.3   2024-06-21 [1] CRAN (R 4.4.0)\n digest        0.6.37  2024-08-19 [1] CRAN (R 4.4.1)\n evaluate      1.0.3   2025-01-10 [1] CRAN (R 4.4.1)\n fastmap       1.2.0   2024-05-15 [1] CRAN (R 4.4.0)\n htmltools     0.5.8.1 2024-04-04 [1] CRAN (R 4.4.0)\n htmlwidgets   1.6.4   2023-12-06 [1] CRAN (R 4.4.0)\n jsonlite      1.8.9   2024-09-20 [1] CRAN (R 4.4.1)\n knitr         1.49    2024-11-08 [1] CRAN (R 4.4.1)\n rlang         1.1.4   2024-06-04 [1] CRAN (R 4.4.0)\n rmarkdown     2.29    2024-11-04 [1] CRAN (R 4.4.1)\n rstudioapi    0.17.1  2024-10-22 [1] CRAN (R 4.4.1)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.4.0)\n xfun          0.50    2025-01-07 [1] CRAN (R 4.4.1)\n yaml          2.3.10  2024-07-26 [1] CRAN (R 4.4.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n:::\n\n\n\n\n</details>\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}